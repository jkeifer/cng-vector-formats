{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b38bed8-9801-4269-8c98-c9c5460503e6",
   "metadata": {},
   "source": [
    "# Reading parquet the hard way\n",
    "\n",
    "Parquet is a cloud-native data format suitable for a variety of data types, including geospatial vector data. It is often described as being \"column-oriented\", as opposed to row-oriented like traditional databases and vector formats (though in reality parquet is really more of a hybrid format, one not strictly columnar).\n",
    "\n",
    "The cloud-native aspect of parquet mainly comes down to two facets of its design:\n",
    "\n",
    "* file metadata is consolidated at the end of a file, and can be read entirely in at most two requests\n",
    "* the metadata contains enough information for clients to selectively read only chunks that may contain targeted rows\n",
    "\n",
    "## Predicate pushdown\n",
    "\n",
    "This \"selective reading\" is done by a process called \"predicate pushdown\". Predicate pushdown enables clients to skip reading potentially massive amounts of data by leveraging things like chunk statistics and chunk bloom filters to restrict reads to only those chunks that _may_ potentially contain target data.\n",
    "\n",
    "Consider an example like this SQL statement:\n",
    "\n",
    "```sql\n",
    "SELECT temperature\n",
    "FROM dataset\n",
    "WHERE\n",
    "  st_intersects(geom, AOI)\n",
    "  AND snow_cover_percent > 0\n",
    "  AND precipitation >= 10\n",
    "  AND land_cover NOT IN (\"rock\", \"urban\")\n",
    "```\n",
    "\n",
    "Here we see the user requesting the `temperature` variable from `dataset` records that have:\n",
    "\n",
    "* a geometry interseting the AOI\n",
    "* any snow cover\n",
    "* some significant precipitation\n",
    "* a non-impervious surface\n",
    "\n",
    "Each of these conditions is called a predicate. Predicate pushdown means using predicates to filter what chunks to read from a file, or to even filter out whole files. File and chunk statistics provide a means of identifying files and chunks that cannot possibly have a record of interest per the query predicates, and skipping those. Of those remaining, bloom filters can provide another efficient check to eliminate chunks that cannot possibly contain target records. What chunks are left are then read and filtered by the client application using the actual values prior to returning the requested data.\n",
    "\n",
    "Note the language \"cannot possibly contain\" is used here specifically: we never want to miss a matching record, so we use techniques that give us means of eliminating chunks with certainty that they will not contain a recod of interest. These techniques come with the trade off that false positives can occur: we can rarely know that chunk _does_ have a target record. Sometimes we then read chunks without target records, because that is the only way to ensure they do not have what we are looking for.\n",
    "\n",
    "Generally though, predicate pushdown is an effective means of eliminating a large amount of data without having to read anything but metadata.\n",
    "\n",
    "## The exercise\n",
    "\n",
    "To get a deeper understanding of parquet and how it works, we're going to examine the [Overture Maps Buildings dataset](https://docs.overturemaps.org/guides/buildings). We will find the geometry for the building we're in for this workshop, the AUT School of Business building.\n",
    "\n",
    "This exercise is perhaps a bit contrived, because we'll start with a rough geometry for the building that we traced from aerial imagery in exercise 1, but the principles we'll be demonstrating are applicable generally for clients wanting to read from parquet, and will show how that process works in some detail. Key to the process will be a focus on using predicate pushdown with geometries, and how we can use less expensive bounding box operations to more effiicently eliminate chunks and records before turning to more expensive real-geometry comparisons.\n",
    "\n",
    "### The `por-que` library\n",
    "\n",
    "To facilitate the learning experience, we'll use the author's custom python parquet parsing library, called `por-que`. This library is different from other, more efficient parsing libraries like `pyarrow`, in that it is designed for the educational experience. A core feature is the parsing of the entire file metadata, data page metadata, and byte structure of the file, which it exposes to users, unlike other tools that keep them hidden away as internal details.\n",
    "\n",
    "`por-que` can export the entire parsed structure to JSON, which can then be loaded, visualized, and explored with the [`ver-por-que` web application](https://teotl.dev/ver-por-que/). We'll see all of this in action through the exercise.\n",
    "\n",
    "But first, let's get some prep out of the way, starting with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "id": "504e551a-cf3c-478e-913c-9af578d591dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "from contextlib import AsyncExitStack\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Self\n",
    " \n",
    "import s3fs\n",
    "\n",
    "from devtools import pprint\n",
    "from por_que import AsyncHttpFile, FileMetadata, ParquetFile\n",
    "from shapely import (\n",
    "    intersection,\n",
    "    from_geojson as shapely_from_geojson,\n",
    "    from_wkb as shapely_from_wkb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7435d-83ed-4f19-a681-929c844f57ad",
   "metadata": {},
   "source": [
    "We'll also define a helper function for zipping async iterators together, which we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "id": "e8e0361f-f6b2-4387-9488-43cdc12bba13",
   "metadata": {},
   "source": [
    "async def azip(*iterators):\n",
    "    \"\"\"This is a silly zip implementation for async iterators that\n",
    "    assumes equal length. Don't copy this. It's not robust.\"\"\"\n",
    "    _iter = iter(iterators)\n",
    "    first, rest = next(_iter), list(_iter)\n",
    "    async for val in first:\n",
    "        yield (val, *[await anext(iterator) for iterator in rest])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657af56f-ef16-48c1-9e37-6160f6cb9aa7",
   "metadata": {},
   "source": [
    "And we'll define a bounding box model, which we will be able to use for all of the bounding box operations in this exercise. Note that we could have used a existing model from a library, but by implementing our own we can see how simple the bound box operations are."
   ]
  },
  {
   "cell_type": "code",
   "id": "e59df241-3fed-43a0-87c6-f3ab291a46c1",
   "metadata": {},
   "source": [
    "@dataclass(slots=True, frozen=True)\n",
    "class BBox:\n",
    "    xmin: int | float\n",
    "    ymin: int | float\n",
    "    xmax: int | float\n",
    "    ymax: int | float\n",
    "\n",
    "    def intersects(self, other: Self) -> bool:\n",
    "        return (\n",
    "            other.xmin < self.xmax\n",
    "            and self.xmin < other.xmax\n",
    "            and other.ymin < self.ymax\n",
    "            and self.ymin < other.ymax\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_linestring(cls, linestring: list[list[int | float]]) -> Self:\n",
    "        xmin = float('inf')\n",
    "        ymin = float('inf')\n",
    "        xmax = float('-inf')\n",
    "        ymax = float('-inf')\n",
    "\n",
    "        try:\n",
    "            for x, y in linestring:\n",
    "                xmin = min(xmin, x)\n",
    "                ymin = min(ymin, y)\n",
    "                xmax = max(xmax, x)\n",
    "                ymax = max(ymax, y)\n",
    "        except Exception:\n",
    "            raise ValueError('Failed to extract bbox')\n",
    "\n",
    "        return cls(xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339dd651-ffbb-468a-b4a0-4ed9197e46df",
   "metadata": {},
   "source": [
    "Lastly, we'll define the geometry we'll be using for this exercise, and we'll use it with our `BBox` class to construct its bounding box. Again, this is the same polygon we delineated in exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "f9072e2b-f09b-4e77-bbac-fe9fd0912bed",
   "metadata": {},
   "source": [
    "geom = json.loads('''{\n",
    "  \"coordinates\": [\n",
    "    [\n",
    "      [\n",
    "        174.76536299052356,\n",
    "        -36.85325730119731\n",
    "      ],\n",
    "      [\n",
    "        174.76501948066357,\n",
    "        -36.85354934760823\n",
    "      ],\n",
    "      [\n",
    "        174.76510987799577,\n",
    "        -36.853728372411425\n",
    "      ],\n",
    "      [\n",
    "        174.76557768418712,\n",
    "        -36.85354844344181\n",
    "      ],\n",
    "      [\n",
    "        174.76544321815658,\n",
    "        -36.85331878474462\n",
    "      ],\n",
    "      [\n",
    "        174.76536299052356,\n",
    "        -36.85325730119731\n",
    "      ]\n",
    "    ]\n",
    "  ],\n",
    "  \"type\": \"Polygon\"\n",
    "}''')\n",
    "geom"
   ]
  },
  {
   "cell_type": "code",
   "id": "53e74c9b-269b-493e-8e49-d0b20b61baa4",
   "metadata": {},
   "source": [
    "geom_bbox = BBox.from_linestring(geom['coordinates'][0])\n",
    "geom_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05994788-c64b-4710-907c-8c4a9abecb06",
   "metadata": {},
   "source": [
    "## Overture Maps parquet data\n",
    "\n",
    "Overture Maps has a number of datasets available in parquet format, including the building footprints dataset we'll be using for this exercise. All are distributed from both AWS and Azure using their respective object storage services. We'll use the files hosted on S3; we can get the S3 url prefix from [the buildings dataset documentation](https://docs.overturemaps.org/guides/buildings)."
   ]
  },
  {
   "cell_type": "code",
   "id": "363ea849-7311-4165-9a34-f35ceb063f3f",
   "metadata": {},
   "source": [
    "s3_url_prefix = 's3://overturemaps-us-west-2/release/2025-09-24.0/theme=buildings/type=building/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa65b3f-7114-4c6d-afb2-8f7beedf01d6",
   "metadata": {},
   "source": [
    "### Listing the files\n",
    "\n",
    "With an S3 prefix, we can use `s3fs` to make a \"filesystem\" instance with which we can see all the files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "130e0d64-8329-4c57-a60c-e81f0d964e29",
   "metadata": {},
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9e9e8bc-de3f-4643-ac8e-4d5d6ddcbb8d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "parquet_parts = fs.ls(s3_url_prefix)\n",
    "parquet_parts"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9870156-ff99-46fe-941d-f132c3ddb15a",
   "metadata": {},
   "source": [
    "len(parquet_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a570f0-b0d8-4f9c-a392-378f8611a046",
   "metadata": {},
   "source": [
    "Wow, that's a lot of files! But it is not unexpected: this is a worldwide building footprints dataset, so it has _a lot_ of records (we'll see exactly how many in a bit). To make it more efficient to access a subset of the dataset, Overture partitions the rows into files around 1 GB in size. So we have a ton of data here, how will we ever find the record for which we are searching?\n",
    "\n",
    "To enable the next step, we need these as HTTPS URLs. Let's transform them."
   ]
  },
  {
   "cell_type": "code",
   "id": "2212884c-e027-4b41-a977-d84274378ffd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "parquet_urls = [\n",
    "    f'https://{bucket}.s3.{\"-\".join(bucket.split('-')[1:])}.amazonaws.com/{key}'\n",
    "    for bucket, key in map(lambda x: x.split('/', 1), parquet_parts)\n",
    "]\n",
    "parquet_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8a9c5-36a1-4d91-ada3-9122d1853de9",
   "metadata": {},
   "source": [
    "## Loading one of these files using `por-que`\n",
    "\n",
    "`por-que` exposes three key classes, each of which we'll be using throughout the rest of this exercise:\n",
    "\n",
    "* `AsyncHttpFile`\n",
    "* `FileMetadata`\n",
    "* `ParquetFile`\n",
    "\n",
    "The first of these, `AsyncHttpFile`, is a helper class that can open an HTTP(S) URL and expose it as a readable filelike object. We need this to allow the other two classes to do their parsing of the parquet files over HTTP.\n",
    "\n",
    "`FileMetadata` is a class that represents the block of metadata at the end of a parquet file. This metadata includes both file level information, like the schema, and chunk level metadata via row groups and their column chunks. Passing a readable filelike object into `FileMetadata.from_reader()` will parse the metadata object from said file.\n",
    "\n",
    "`ParquetFile` is a class that represents then entire physical and logical structure of a parquet file. This includes `FileMetadata`, but also all data page locations and metadata. `ParquetFile` and its nested `DataPage` objects allow reading data from the file. `ParquetFile.from_reader()`, similar to above, will parse the full structure and metadata from a passed in readable filelike object.\n",
    "\n",
    "Let's see how we can use `AsyncHttpFile` with `ParquetFile` and some of what the latter exposes. Note that creating the `ParquetFile` instances has to make many random reads within the file, so this is not a fast process, especially over the network (typically taking around 30 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "id": "03b85ee7-c3b2-4851-908e-ebb17b17a1c3",
   "metadata": {},
   "source": [
    "async with AsyncHttpFile(parquet_urls[0]) as hf:\n",
    "    pf = await ParquetFile.from_reader(hf, parquet_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "id": "08b862fb-3516-46a6-a6e4-84f4118ce66e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pprint(pf.metadata.schema_root)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf008c18-b068-487a-a81a-faa69dbaa0ff",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pprint(pf.metadata.row_groups[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f431a-b723-49cc-b0f8-7aabdeb18836",
   "metadata": {},
   "source": [
    "### Visualizing the `ParquetFile` structure\n",
    "\n",
    "We can dump our `ParquetFile` model instance to JSON, and then we can load that into a static web application called [ver-por-que](https://teotl.dev/ver-por-que). The app will show the structure via an interactive visualization.\n",
    "\n",
    "So let's first write our JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ee6d53d-236f-4a1b-a818-9cf6ab71c049",
   "metadata": {},
   "source": [
    "Path('./buildings.parquet.json').write_text(pf.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46a963-0b21-404e-a893-a4155b34f0b8",
   "metadata": {},
   "source": [
    "Now, download the json file (right-click on the file in the file explorer sidebar), then browse to [ver-por-que](https://teotl.dev/ver-por-que) and load the JSON file.\n",
    "\n",
    "Key things to notice:\n",
    "\n",
    "* how data page region is logically organized into row groups and column chunks, but all the row group and column chunk information comes from the file metadata\n",
    "* the schema is not a flat set of columns: it has nested data structures, like the bbox columns\n",
    "* the general layout of the file bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f596923-f9cf-4bb4-a165-43162a982565",
   "metadata": {},
   "source": [
    "### File-level geo metadata\n",
    "\n",
    "Another thing to note is that the parquet specification does not have file-level statistics or bloom filters--such information is available (optionally) at the column chunk (and sometimes data page) level, but not the file level.\n",
    "\n",
    "To faciliate more performant reading by allowing filtering at the file level, convention dictates writing geospatial metadata to the top-level of the file using the flexible \"key/value\" metadata in the file metadata structure. This geospatial metadata is written in JSON format, and includes the bounding box around all geometries in th file.\n",
    "\n",
    "We can view that metadata like so:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5493c348-51ec-46d5-99e1-432beba570a7",
   "metadata": {},
   "source": [
    "json.loads([kv for kv in pf.metadata.key_value_metadata if kv.key == 'geo'][0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196bf07e-728f-4f2b-bcda-beef568566ed",
   "metadata": {},
   "source": [
    "## Filtering files\n",
    "\n",
    "Now that we know how to determine the bounding box for a file's geometries, we can take a first pass at eliminating data we'll need to read by finding only those files that have a bounding box that intersects our geometry's bounding box.\n",
    "\n",
    "### Reading the metadata for all files\n",
    "\n",
    "Above, we read the geospatial metadata for a file, but we need to do it for all files. Reading the metadata can be slow due to network round trips and latency. To make reading the metadata across all files possible in a somewhat reasonable amount of time we can leverage parallelism, but that comes at the cost of code complexity.\n",
    "\n",
    "In essence, the code below is creating an `AsyncHttpFile` instance for every file in our list. We open each of those instances within a context manager, so they will be automatically closed once we leave this context (because each consumes a not-insignificant amount of memory while they remain open). With each of those `AsyncHttpFile` instances we can create a corresponding `FileMetadata` instance via an async function call. We put all those async calls into a list, so we can use `asyncio.gather` to run them in parallel, and we `await` the completion of the gather to get all the produced `FileMetadata` instances. Again, parallelism allows us to make many HTTP requests for the data at once, instead of having to wait as we would when running requests serially.\n",
    "\n",
    "These `FileMetadata` instances we zip up with the corresponding URLs, and we use the resulting tuples to construct a dictionary keyed on the file URL. This gives us a mapping of a file's URL to its `FileMetadata` instance.\n",
    "\n",
    "We'll also collect and output the total number of bytes in this dataset summed across all files.\n",
    "\n",
    "**Note**: this process is rather slow. We have to download ~300 MB, parse and instantiate all of the metadata data structures. In testing this took anywhere from 2 to 5 minutes... If this takes too long, feel free to skip this part by reviewing the outputs in the completed notebook, and copy the `urls_that_intersect` values and define that before proceeding to steps that need those URLs. "
   ]
  },
  {
   "cell_type": "code",
   "id": "31354cba-84dc-4b6a-8c71-e44e6084771c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "fm_tasks = []\n",
    "total_bytes = 0\n",
    "async with AsyncExitStack() as stack:\n",
    "    for url in parquet_urls:\n",
    "        f = await stack.enter_async_context(AsyncHttpFile(url))\n",
    "        total_bytes += f.size\n",
    "        fm_tasks.append(asyncio.create_task(FileMetadata.from_reader(f)))\n",
    "    fms = dict(zip(parquet_urls, await asyncio.gather(*fm_tasks)))\n",
    "\n",
    "print(f'{total_bytes:_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20f98c-4327-4f7f-80fe-909b4b589738",
   "metadata": {},
   "source": [
    "Let's also see how many rows we have in this dataset across all the files."
   ]
  },
  {
   "cell_type": "code",
   "id": "167f0cda-6cf1-43f8-9f5c-c4f445baa33e",
   "metadata": {},
   "source": [
    "total_rows = 0\n",
    "for fm in fms.values():\n",
    "    total_rows += fm.row_count\n",
    "\n",
    "print(f'{total_rows:_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78809879-c207-4a58-8836-2395678bd884",
   "metadata": {},
   "source": [
    "### Finding intersecting files by their bounding boxes\n",
    "\n",
    "We can iterate through our file URLs and their `FileMetadata` instances to build a `BBox` for each from the geosptial metadata. We do the same `json.loads()` call on the key/value metadata entry with the `geo` key that we saw above; from the resulting dictionary we can build a `BBox` from the `['columns']['geometry']['bbox']` fields.\n",
    "\n",
    "Once we have a file's `BBox`, we can check for an intersection with out geometry's `BBox` and note where we find one. At the end we will have a list of the URLs for all files that are worthy of a closer look. Any that get filtered out we can know with certainty do not contain rows of interest, so we can safely disregard them."
   ]
  },
  {
   "cell_type": "code",
   "id": "16882dbe-b03a-4453-8ac6-9bb7d497096b",
   "metadata": {},
   "source": [
    "urls_that_intersect = []\n",
    "for fm_url, fm in fms.items():\n",
    "    kv_metadata = json.loads(\n",
    "        [\n",
    "            kv for kv in fm.key_value_metadata\n",
    "            if kv.key == 'geo'\n",
    "        ][0].value,\n",
    "    )\n",
    "    bbox = BBox(*kv_metadata['columns']['geometry']['bbox'])\n",
    "    if bbox.intersects(geom_bbox):\n",
    "        print(bbox)\n",
    "        urls_that_intersect.append(fm_url)\n",
    "\n",
    "print(geom_bbox)\n",
    "urls_that_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d365e-d532-498e-abcd-1b172c494f21",
   "metadata": {},
   "source": [
    "**If you skipped this section because it was too slow, define the `urls_that_intersect` array here before proceeding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bab018-d161-4bcd-9c6b-deffeea011e3",
   "metadata": {},
   "source": [
    "## Finding intersecting row groups\n",
    "\n",
    "We've successfully filtered down the file set to a much smaller number, but we can still filter the data we need to read even further. To do so, we can perform the same bounding box filtering we did at the file level, but at the row group level using the column chunk statistics across our bounding box columns.\n",
    "\n",
    "Let's walk through this process for one file, and once we see how that process comes together we can create a single routine to perform the checks across each file in our filtered set.\n",
    "\n",
    "To begin, we'll grab the `FileMetadata` instance for one of our matching URLs. From that we can start looking into the logical structures of the file, the row groups and column chunks."
   ]
  },
  {
   "cell_type": "code",
   "id": "506b24ab-2579-4c76-955e-3d0086fc2f2a",
   "metadata": {},
   "source": [
    "fm = fms[urls_that_intersect[0]]"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e37602a-2e27-4a7b-8e29-0f00d8ab5c66",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "bbox_min_cc0 = fm.row_groups[0].column_chunks['bbox.xmin']\n",
    "pprint(bbox_min_cc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70550087-f482-4ee9-af78-5990faa348b0",
   "metadata": {},
   "source": [
    "We see the structure of the file starting to take shape:\n",
    "\n",
    "* Files have rows split up into row groups\n",
    "* Files have columns, but these are not accessed directly\n",
    "* Row groups have a column chunk for each column in the file\n",
    "\n",
    "Parquet is not truly column-oriented! It is column oriented within row groups, but within a file the organization is something of a hybrid between column and row-orientation.\n",
    "\n",
    "What does this mean for us? We cannot just read a column. But that's okay, because if we identify column chunks that might contain data we're looking for then we can identify what row groups definitely don't have records we want, and we have a means of filtering down the data we need to read.\n",
    "\n",
    "But how do we do this? We can use the column chunk statistics. Specifically the min and max values."
   ]
  },
  {
   "cell_type": "code",
   "id": "7bcccb0e-270f-4eea-9f9e-94945c0ce9df",
   "metadata": {},
   "source": [
    "bbox_min_cc0.statistics.min_value"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bc2927a-5591-4f0e-a4e7-28a52f223333",
   "metadata": {},
   "source": [
    "bbox_min_cc0.statistics.max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1bc2b-14b3-497d-ac63-95084cb44236",
   "metadata": {},
   "source": [
    "These values aren't super helpful looking, are they? What do they even mean?\n",
    "\n",
    "To understand how to interpret these raw bytes, we need to do two things:\n",
    "\n",
    "* Convert them to the column's physical type\n",
    "* Convert the physical type value to the column's logical type\n",
    "\n",
    "To get the type information for a column, we can use the `FileMetadata`'s schema. To make it easier to access the specific schema element for this column, each column chunk has a link to it's schema element in its metadata."
   ]
  },
  {
   "cell_type": "code",
   "id": "6e48e736-b9b9-4097-9315-9748e26a608d",
   "metadata": {},
   "source": [
    "schema = bbox_min_cc0.metadata.schema_element\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b3666-c9d9-4da9-a80a-962a840c9174",
   "metadata": {},
   "source": [
    "We can explicitly get the physical and logical types from this schema element (which are also contained in its string representation above):"
   ]
  },
  {
   "cell_type": "code",
   "id": "89b18d92-1208-4da2-bac2-48eac17dd3c2",
   "metadata": {},
   "source": [
    "schema.type"
   ]
  },
  {
   "cell_type": "code",
   "id": "da0933f7-dbd0-4218-928f-c3664c078033",
   "metadata": {},
   "source": [
    "schema.get_logical_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87dc8f1-a4a6-4cbe-8a6f-d59f6d09a6ea",
   "metadata": {},
   "source": [
    "In this case, we get a fairly expected answer: the physical type of bounding box coordinates is float, and no additional logical type conversion need happen to use the float values (logical types are super useful in other cases, like telling us a bytes physical type column is a geometry, or encoding other special types like decimals, datetimes, JSON, etc.).\n",
    "\n",
    "With this type information we can convert our raw bytes values into something more useful. In fact, the schema element has conversion helper methods on it for just this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "id": "95fdb199-9d61-4633-8aec-234993b506e8",
   "metadata": {},
   "source": [
    "schema.physical_to_logical_type(schema.bytes_to_physical_type(bbox_min_cc0.statistics.max_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb73c5-cf8a-430a-9e11-9d2b621e16ec",
   "metadata": {},
   "source": [
    "What's more, the statistics object itself can convert the min and max values for us (using these same convenience methods under the covers):"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1eff648-e997-4425-a0fd-a7c2f9c2d144",
   "metadata": {},
   "source": [
    "{\n",
    "    'min_value': bbox_min_cc0.statistics.converted_min_value,\n",
    "    'max_value': bbox_min_cc0.statistics.converted_max_value,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade45c0-45b4-403b-8b26-41dc3790ee25",
   "metadata": {},
   "source": [
    "Now that we can get this min/max statistics in a useful form, we can build up a bounding box for each row group. Let's make a function to construct a `BBox` instance for a given row group:"
   ]
  },
  {
   "cell_type": "code",
   "id": "13e9d367-cd67-4014-9268-a9dad33bfebb",
   "metadata": {},
   "source": [
    "def get_rg_bbox(row_group) -> BBox:\n",
    "    return BBox(\n",
    "        row_group.column_chunks['bbox.xmin'].statistics.converted_min_value,\n",
    "        row_group.column_chunks['bbox.ymin'].statistics.converted_min_value,\n",
    "        row_group.column_chunks['bbox.xmax'].statistics.converted_max_value,\n",
    "        row_group.column_chunks['bbox.ymax'].statistics.converted_max_value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4480d-9d97-42bf-8e54-b78ad1a1abb0",
   "metadata": {},
   "source": [
    "We can try it out and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f5fafc0-a397-41af-9a35-35b1c9e31eb6",
   "metadata": {},
   "source": [
    "get_rg_bbox(fm.row_groups[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216ba12-9f31-4fd6-ad59-4aecc031f4dd",
   "metadata": {},
   "source": [
    "Cool, that works! Let's run it on every row group of our `FileMetadata` instance and check for any intersections"
   ]
  },
  {
   "cell_type": "code",
   "id": "0acc9590-8c4a-49a2-b377-f324a918f68b",
   "metadata": {},
   "source": [
    "intersecting_rgs = [rg for rg in fm.row_groups if get_rg_bbox(rg).intersects(geom_bbox)]\n",
    "len(intersecting_rgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be5e9b-d228-4275-b826-33dec7dbe8bf",
   "metadata": {},
   "source": [
    "Look at that! We've successfully filtered down the rows we need to be concered about significantly, _and we haven't even read any data yet_.\n",
    "\n",
    "Can we keep using metadata to filter further? Like, does the geometry column have metadata that could be used in a similar manner?"
   ]
  },
  {
   "cell_type": "code",
   "id": "94632294-98f0-4989-bbfd-2c91e815e669",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pprint(intersecting_rgs[0].column_chunks['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6aa59-f9a6-4bf0-88d5-7780f146da5b",
   "metadata": {},
   "source": [
    "Hmm, that's not super helpful looking. It might be time to say we've done as much as we can without reading data, and start fetching rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df479f29-d730-42e4-a24b-d1ccdcb831e1",
   "metadata": {},
   "source": [
    "## Reading and filtering rows\n",
    "\n",
    "To read rows we need to get full `ParquetFile` instances for our intersecting files. Once we have a `ParquetFile` instance, we can read its column chunks.\n",
    "\n",
    "We're going to need a `ParquetFile` instance per intersecting file. Because instantiating them is a time consuming operation, let's instantiate them all in parallel here, building them up into a dictionary keyed on the file URL. Then we can just grab one of them to use for the rest of this section as we prove out our process (making sure it is for the same file as the `FileMetadata` instance we were using above)."
   ]
  },
  {
   "cell_type": "code",
   "id": "61aacfd0-3a3b-4925-a6cc-113b841e9e55",
   "metadata": {},
   "source": [
    "pf_tasks = []\n",
    "async with AsyncExitStack() as stack:\n",
    "    for url in urls_that_intersect:\n",
    "        f = await stack.enter_async_context(AsyncHttpFile(url))\n",
    "        pf_tasks.append(asyncio.create_task(ParquetFile.from_reader(f, url)))\n",
    "    pfs = dict(zip(urls_that_intersect, await asyncio.gather(*pf_tasks)))\n",
    "\n",
    "pf = pfs[urls_that_intersect[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba2fe8-fd38-4d3f-bfde-53c18fb42f05",
   "metadata": {},
   "source": [
    "With a `ParquetFile` instance, we can find all four of the `bbox` column chunks in one of our intersected row groups. We do this by iterating through all column chunks in the file, keeping those that are part of our intersected row group index (given by that row group's `ordianal` property) that have a schema path starting with `bbox`. The column chunks we collect here we'll put into a dictionary, keyed on the column chunk path in the schema (e.g., `bbox.min`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "c7cde1e9-fa66-42d0-8bdc-36d948689826",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "bbox_chunks = {\n",
    "    cc.path_in_schema: cc\n",
    "    for cc in pf.column_chunks\n",
    "    if cc.row_group == intersecting_rgs[0].ordinal and cc.path_in_schema.startswith('bbox')\n",
    "}\n",
    "bbox_chunks.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430ff32-2811-4fc2-82fe-8d8e9cfecd6e",
   "metadata": {},
   "source": [
    "Now that we have identified the relevant column chunks and inspected what they are, let's take a closer look at one to see other metadata it has."
   ]
  },
  {
   "cell_type": "code",
   "id": "8c97ad5e-ac57-4e23-8bf5-68adc46341c8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pprint(bbox_chunks['bbox.xmin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6bbb8-1264-4dd5-bcdc-4c7b47ef2b94",
   "metadata": {},
   "source": [
    "We see from the above that the name of the type for our column chunk instances is `PhysicalColumnChunk`. That's because these column chunks are not the column chunk metadata in the file metadata, but are instead representative of all the physical byte ranges in the file that make up this \"column chunk\" abstraction within the file. This includes the column chunk metadata from the file metadata, any indices in the file metadata (column index or offset index), any dictionary page, or, most prominently, the data pages that store the data for the column chunk.\n",
    "\n",
    "As a result, this `PhysicalColumnChunk` type has a method `parse_all_data_pages()`, which we can use read all data pages in the column chunk and parse the data values from them. A dictionary page, if present, will also be read so the data pages can be correctly decompressed.\n",
    "\n",
    "Let's use `parse_all_data_pages()` with each of our bbox chunks to get all the bbox values for our target row group. We have to have a readable filetype object to pass in because we need to read file data, so we'll use the `AsyncHttpFile` class again and pass in an open instance of that class. The data values we read we'll zip together into a four-tuple like `(xmin, ymin, xmax, ymax)`; this tuple provides us a data structure we can use to instantiate `BBox` instances for each row in a later step."
   ]
  },
  {
   "cell_type": "code",
   "id": "344881fc-e76c-4afc-98e0-d3d5e84903ef",
   "metadata": {},
   "source": [
    "async with AsyncHttpFile(urls_that_intersect[0]) as hf:\n",
    "    bbox_tuples = [bbox_tuple async for bbox_tuple in azip(\n",
    "        bbox_chunks['bbox.xmin'].parse_all_data_pages(hf),\n",
    "        bbox_chunks['bbox.ymin'].parse_all_data_pages(hf),\n",
    "        bbox_chunks['bbox.xmax'].parse_all_data_pages(hf),\n",
    "        bbox_chunks['bbox.ymax'].parse_all_data_pages(hf),\n",
    "    )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656225e-d917-4b37-bdac-a7801b62f4aa",
   "metadata": {},
   "source": [
    "Let's take a look at what the first four rows look like."
   ]
  },
  {
   "cell_type": "code",
   "id": "a88e68f6-b9ad-468e-978c-b9d77779c53e",
   "metadata": {},
   "source": [
    "bbox_tuples[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b532086-085e-4906-9729-98064a450f80",
   "metadata": {},
   "source": [
    "This result, it might be unexpected. Each data value we see is a three-tuple like `(float, int, int)`. The float values, if not obviously, are our actual data values. The integer values are the definition and repetition levels for each value.\n",
    "\n",
    "Exactly what the definition and repetition levels are and how they work is outside the scope of this exercise; the short version is they are used in reconstructing nested types like maps, arrays, and structs, by providing the necessary state to determine when and where within a data tree to end/start a data structure or insert nulls. To learn more about how this works, review the three-part blog post series on the Apache Arrow blog ([part 1](https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/), [part 2](https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/), and [part 3](https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/)) or dig into the `por_que.structuring` code.\n",
    "\n",
    "For now, just note that for our purposes we only need the first element of these tuples, which we can get using something like the generator expression `(i[0] for i in bbox_tuple)`. Let's use that when iterating through each of our bbox tuples, constructing a `BBox` instance from each tuple's extracted values and checking the intersection with our geometry's `BBox` instance. We'll print out the index of each row that intersects, if any."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ddac560-c1b1-458b-aad0-152396eb5aa6",
   "metadata": {},
   "source": [
    "for row_index, bbox_tuple in enumerate(bbox_tuples):\n",
    "    if BBox(*(i[0] for i in bbox_tuple)).intersects(geom_bbox):\n",
    "        print(row_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1960c-aa19-4edf-8810-bb7be90768ec",
   "metadata": {},
   "source": [
    "Now we have all the steps we need to identify rows that intersect our search bounding box. Let's put all these steps together so we can iterate through all the intersecting URLs and find any and all rows with intersecting bounding boxes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6973b2d-ea3f-4985-b8b7-f3f57dabb479",
   "metadata": {},
   "source": [
    "## Putting this all together\n",
    "\n",
    "Let's make a function that, given a `ParquetFile` instance, will search for intersecting row group bounding boxes, then search those row groups for rows with intersecting bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "id": "7a51b38e-ce38-48b5-a7ce-fedeff825951",
   "metadata": {},
   "source": [
    "async def find_intersecting_rows(pf: ParquetFile) -> dict[int, list[int]]:\n",
    "    matched_rows = {}\n",
    "    for rg in pf.metadata.row_groups:\n",
    "        if not get_rg_bbox(rg).intersects(geom_bbox):\n",
    "            continue\n",
    "\n",
    "        bbox_chunks = {\n",
    "            cc.path_in_schema: cc\n",
    "            for cc in pf.column_chunks\n",
    "            if cc.row_group == rg.ordinal and cc.path_in_schema.startswith('bbox')\n",
    "        }\n",
    "\n",
    "        # we should be able to use the same AsyncHttpFile but\n",
    "        # some as-of-yet-not-understood issue is causing a\n",
    "        # ServerDisconnectedError when we reuse\n",
    "        async with AsyncHttpFile(pf.source) as hf:\n",
    "            bbox_tuples = [bbox_tuple async for bbox_tuple in azip(\n",
    "                bbox_chunks['bbox.xmin'].parse_all_data_pages(hf),\n",
    "                bbox_chunks['bbox.ymin'].parse_all_data_pages(hf),\n",
    "                bbox_chunks['bbox.xmax'].parse_all_data_pages(hf),\n",
    "                bbox_chunks['bbox.ymax'].parse_all_data_pages(hf),\n",
    "            )]\n",
    "        \n",
    "        for row_index, bbox_tuple in enumerate(bbox_tuples):\n",
    "            if BBox(*(i[0] for i in bbox_tuple)).intersects(geom_bbox):\n",
    "                try:\n",
    "                    matched_rows[rg.ordinal].append(row_index)\n",
    "                except KeyError:\n",
    "                    matched_rows[rg.ordinal] = [row_index]\n",
    "\n",
    "    return matched_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511ae91-cc89-49e6-9a22-6ad6aa354cdb",
   "metadata": {},
   "source": [
    "Now let's use our `find_intersecting_rows` function on each of our intersecting `ParquetFile` instances. We'll track the outputs in a nested dictionary structure mapping file URL to row group index to intersected row indices."
   ]
  },
  {
   "cell_type": "code",
   "id": "22795e4d-b19a-4fe3-a772-2f0e5c697e84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "matched_rows: dict[str, dict[int, list[int]]] = dict(\n",
    "    zip(\n",
    "        urls_that_intersect,\n",
    "        await asyncio.gather(*[\n",
    "            asyncio.create_task(find_intersecting_rows(pf))\n",
    "            for pf in pfs.values()\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "matched_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d158d-8b62-41ce-a925-c471a3efd7d8",
   "metadata": {},
   "source": [
    "As only one row group in one file had intersections, let's extract the key bits of information here out into discrete variables, to keep the following code simpler. Specifically, we'll define vars for the file URL, the row group index, and the intersected row indices."
   ]
  },
  {
   "cell_type": "code",
   "id": "6527aa2a-a1d2-4590-841f-248774e4b47f",
   "metadata": {},
   "source": [
    "file_url, row_group_index, row_indices = [(url, k, v) for url, d in matched_rows.items() if d for k, v in d.items()][0]\n",
    "file_url, row_group_index, row_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a013016-cb07-46a4-938d-37f59886d12f",
   "metadata": {},
   "source": [
    "## Reading data for the intersecting rows\n",
    "\n",
    "### A simple string column\n",
    "\n",
    "We can use the row group index and row indices to read other column chunks that contain other data for the rows. Let's take a look at how that works with the `names.primary` column, to see if that provides any insight into the intersected rows.\n",
    "\n",
    "To do this, we need to first find the column chunk for this `name.primary` column that is within our target row group. We can iterate through the target `ParquetFile`'s column chunks until we find the one that matches."
   ]
  },
  {
   "cell_type": "code",
   "id": "450aa39c-4d02-45d6-92de-a782f0cabaef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for cc in pfs[file_url].column_chunks:\n",
    "    if cc.row_group == row_group_index and cc.path_in_schema == 'names.primary':\n",
    "        primary_name_chunk = cc\n",
    "        break\n",
    "\n",
    "pprint(primary_name_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eaf6d2-0694-4b54-9dd7-3ca72e6a3e7d",
   "metadata": {},
   "source": [
    "With the column chunk identified, we can parse all its data pages to get all its rows."
   ]
  },
  {
   "cell_type": "code",
   "id": "d62fda96-877a-46b4-ac10-c464a2ff3e36",
   "metadata": {},
   "source": [
    "async with AsyncHttpFile(file_url) as hf:\n",
    "    name_rows = [name async for name in primary_name_chunk.parse_all_data_pages(hf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bc2ed-76d6-45f4-a7dd-63afc99c9278",
   "metadata": {},
   "source": [
    "Now it's simply a matter of grabbing each row with our target row indices."
   ]
  },
  {
   "cell_type": "code",
   "id": "247f4f83-a34c-43b7-8091-7035ecbadd37",
   "metadata": {},
   "source": [
    "for row in row_indices:\n",
    "    print(name_rows[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a33c6-3f33-435f-ba4b-3bf682aa6445",
   "metadata": {},
   "source": [
    "Easy yeah?\n",
    "\n",
    "### Reading the geometries\n",
    "\n",
    "We can do this same process again to get access to the target row geometries. So let's do that, starting again by finding the column chunk for the `geometry` column in our target row group."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d954bf0-b050-459f-aea5-dcc6d43220ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for cc in pfs[file_url].column_chunks:\n",
    "    if cc.row_group == row_group_index and cc.path_in_schema == 'geometry':\n",
    "        geom_chunk = cc\n",
    "        break\n",
    "\n",
    "pprint(geom_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e6a53-0548-430d-9916-9f197218729a",
   "metadata": {},
   "source": [
    "Now we can read the rows from the column chunk. This time we'll combine that operation with the row filtering as well, so we just collect our target geometries. Let's also print those out and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "id": "e8cf609c-12e7-4824-a567-4041f0d7476c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "async with AsyncHttpFile(file_url) as hf:\n",
    "    geom_rows = [[geom async for geom in geom_chunk.parse_all_data_pages(hf)][i] for i in row_indices]\n",
    "geom_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723bc30-797c-4ac5-aa61-225c81cc2e69",
   "metadata": {},
   "source": [
    "That doesn't look too useful, does it?\n",
    "\n",
    "The type for this column is `BYTE_ARRAY`, and we have no logical type specified in the schema for this column. The addition of parquet `GEOMETRY` and `GEOGRAPHY` logical types is [really fairly recent](https://github.com/apache/parquet-format/pull/240). It appears this file does not use either of those types, but instead relies on the older geoparquet specfication that defines the convention of encoding geometries in `BYTE_ARRAY` format with no logical type.\n",
    "\n",
    "In fact, if we go back to the file metadata and the geospatial key/value metadata, we can see where this encoding was specified:"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbf990b4-57ef-42be-a0bf-e43ec89ec321",
   "metadata": {},
   "source": [
    "json.loads([kv for kv in pfs[file_url].metadata.key_value_metadata if kv.key == 'geo'][0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bc30a-af7e-43d4-b028-2fef3fe968ff",
   "metadata": {},
   "source": [
    "The `geometry` column we see here is encoded as WKB! Let's take a second look at the values in hex format and see if WKB seems a reasonable interpretation of these byte values. We have to extract just the value from our `(value, definition, repetition)` tuples though, so we'll do that first."
   ]
  },
  {
   "cell_type": "code",
   "id": "41c6b572-6769-4fbf-966d-5b9934dbac45",
   "metadata": {},
   "source": [
    "geoms = [row[0] for row in geom_rows]\n",
    "for _geom in geoms:\n",
    "    print(_geom.hex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdebae-34e7-40a8-8461-387796ad4713",
   "metadata": {},
   "source": [
    "What do you think? Do these look like they could be WKB values?\n",
    "\n",
    "We can use the `from_wkb()` function from shapely to parse these values into geometries, if indeed they are valid. That function is imported here as `shapely_from_wkb()`; let's try it out! We'll collect all the resulting shapes in a dictionary keyed on the row index."
   ]
  },
  {
   "cell_type": "code",
   "id": "7ecb5123-7667-44f8-8515-78a92e850dae",
   "metadata": {},
   "source": [
    "shapely_geoms = {}\n",
    "for index, _geom in zip(row_indices, geoms):\n",
    "    shapely_geom = shapely_from_wkb(_geom)\n",
    "    shapely_geoms[index] = shapely_geom\n",
    "    display(shapely_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217f5d9-b64c-4f54-9212-20c7b3d7e910",
   "metadata": {},
   "source": [
    "Hmm, interesting. It may or may not be obvious which of these is our building of interest, but either way let's treat this as a problem to solve robustly. We've done really inexpensive bounding box comparisons up until now to filter down the set of possibilities as cheaply as possible. Now that we have some actual contenders, we should switch to th more expensive comparison of the geometries themselves.\n",
    "\n",
    "To facilitate this comparison, let's also load our geometry into shapely."
   ]
  },
  {
   "cell_type": "code",
   "id": "9802c6ae-8c0a-4000-85e8-7d2af6b72bba",
   "metadata": {},
   "source": [
    "our_geom = shapely_from_geojson(json.dumps(geom))\n",
    "display(our_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7cf259-4343-42a8-8dc2-2d4a54fed9d8",
   "metadata": {},
   "source": [
    "With shapely geometries on either side, we can find all geometries that intersect our geometry."
   ]
  },
  {
   "cell_type": "code",
   "id": "60b00c8e-6362-4a16-8d11-aaf53bf979d7",
   "metadata": {},
   "source": [
    "for _geom in shapely_geoms.values():\n",
    "    if _geom.intersects(our_geom):\n",
    "        display(_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b943a22-5eec-4ea7-b730-af9963036c5d",
   "metadata": {},
   "source": [
    "Hmm, well, we've narrowed it down. But still, we need to do better. How can we estimate similarity? Perhaps if we calculate the actual intersection as a geometry, we could measure its area? The larger the intersecting area, the more likely the geometry is our target building? After all, we might expect some overlaps around the edges that could cause intersecting slivers, but we should not have buildings overlapping in the Overture dataset.\n",
    "\n",
    "So let's try that out and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "id": "1850c9d0-5818-41f4-94d7-46969a66a5e8",
   "metadata": {},
   "source": [
    "max_intersection_area = 0\n",
    "max_intersecting_geom = None\n",
    "max_intersecting_index = None\n",
    "\n",
    "for index, _geom in shapely_geoms.items():\n",
    "    area = intersection(_geom, our_geom).area\n",
    "    if area > max_intersection_area:\n",
    "        max_intersection_area = area\n",
    "        max_intersecting_geom = _geom\n",
    "        max_intersecting_index = index\n",
    "\n",
    "display(max_intersecting_geom)\n",
    "print(f'The row that most intersects our geometry is {max_intersecting_index} of row group {row_group_index} in {file_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf16e2a-9a31-4b7d-9b1b-499379ebd1a1",
   "metadata": {},
   "source": [
    "Look at that! We've narrowed it down to a single row. Does this look like it is the right row?\n",
    "\n",
    "## Questions\n",
    "\n",
    "* How effective was parquet's support for predicate pushdown in our search? Can the access efficiency be quantified for this particular scenario?\n",
    "* Using what you learned through this exercise, can you classify what types of problems parquet supports well?\n",
    "* What about what parquet would be bad at? What are the trade-offs of parquet? What might be done to mitigate its trade-offs? What cannot be mitigated?\n",
    "* Is parquet a true cloud-native format? Some argue that it is not. Can you make a case for why it is? What about why it might not be?\n",
    "* What would it take to write a generic client for this access pattern? What about others? What would it take to make a truly general purpose query engine for parquet?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "exercise_version": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}